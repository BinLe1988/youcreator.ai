"""
å¼€æºæ¨¡å‹ç®¡ç†å™¨
é›†æˆå¤šç§å¼€æºAIæ¨¡å‹
"""

import os
import torch
import asyncio
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path
import gc

logger = logging.getLogger(__name__)


class OpenSourceModelManager:
    """å¼€æºæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models: Dict[str, Any] = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.models_dir = Path(os.getenv("MODEL_CACHE_DIR", "./models"))
        self._initialized = False
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            "text": {
                "model_id": os.getenv("TEXT_MODEL_ID", "Qwen/Qwen2-1.5B-Instruct"),
                "local_path": self.models_dir / "text" / "qwen",
                "type": "text-generation"
            },
            "image": {
                "model_id": os.getenv("IMAGE_MODEL_ID", "runwayml/stable-diffusion-v1-5"),
                "local_path": self.models_dir / "image" / "sd-1.5",
                "type": "text-to-image"
            },
            "music": {
                "model_id": os.getenv("MUSIC_MODEL_ID", "facebook/musicgen-small"),
                "local_path": self.models_dir / "music" / "musicgen-small",
                "type": "text-to-audio"
            },
            "code": {
                "model_id": os.getenv("CODE_MODEL_ID", "codellama/CodeLlama-7b-Python-hf"),
                "local_path": self.models_dir / "code" / "codellama",
                "type": "text-generation"
            }
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        if self._initialized:
            return
        
        logger.info("ğŸš€ åˆå§‹åŒ–å¼€æºAIæ¨¡å‹...")
        
        try:
            # æ£€æŸ¥GPUå†…å­˜
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                logger.info(f"ğŸ® GPU: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
                
                if gpu_memory < 6:
                    logger.warning("âš ï¸ GPUå†…å­˜ä¸è¶³6GBï¼Œå°†ä½¿ç”¨CPUæˆ–è¾ƒå°æ¨¡å‹")
            
            # åˆå§‹åŒ–æ–‡æœ¬æ¨¡å‹
            await self._init_text_model()
            
            # åˆå§‹åŒ–å›¾åƒæ¨¡å‹
            await self._init_image_model()
            
            # åˆå§‹åŒ–éŸ³ä¹æ¨¡å‹ (å¯é€‰)
            await self._init_music_model()
            
            # åˆå§‹åŒ–ä»£ç æ¨¡å‹
            await self._init_code_model()
            
            self._initialized = True
            logger.info("âœ… æ‰€æœ‰å¼€æºæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æœåŠ¡ç»§ç»­è¿è¡Œ
    
    async def _init_text_model(self):
        """åˆå§‹åŒ–æ–‡æœ¬ç”Ÿæˆæ¨¡å‹"""
        try:
            logger.info("ğŸ“ åŠ è½½æ–‡æœ¬ç”Ÿæˆæ¨¡å‹...")
            
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            
            config = self.model_configs["text"]
            model_path = str(config["local_path"]) if config["local_path"].exists() else config["model_id"]
            
            # ä½¿ç”¨pipelineç®€åŒ–åŠ è½½
            self.models["text"] = pipeline(
                "text-generation",
                model=model_path,
                tokenizer=model_path,
                device=0 if self.device == "cuda" and torch.cuda.is_available() else -1,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True,
                model_kwargs={
                    "low_cpu_mem_usage": True,
                    "use_cache": True
                }
            )
            
            logger.info("âœ… æ–‡æœ¬æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨å¤‡ç”¨çš„è½»é‡çº§æ¨¡å‹
            try:
                from transformers import pipeline
                self.models["text"] = pipeline(
                    "text-generation",
                    model="gpt2",
                    device=-1  # å¼ºåˆ¶ä½¿ç”¨CPU
                )
                logger.info("âœ… ä½¿ç”¨å¤‡ç”¨æ–‡æœ¬æ¨¡å‹ (GPT-2)")
            except:
                self.models["text"] = None
    
    async def _init_image_model(self):
        """åˆå§‹åŒ–å›¾åƒç”Ÿæˆæ¨¡å‹"""
        try:
            logger.info("ğŸ¨ åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹...")
            
            from diffusers import StableDiffusionPipeline
            import torch
            
            config = self.model_configs["image"]
            model_path = str(config["local_path"]) if config["local_path"].exists() else config["model_id"]
            
            # æ£€æŸ¥GPUå†…å­˜ï¼Œå†³å®šä½¿ç”¨ç²¾åº¦
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                use_fp16 = gpu_memory > 6  # 6GBä»¥ä¸Šä½¿ç”¨FP16
            else:
                use_fp16 = False
            
            self.models["image"] = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if use_fp16 else torch.float32,
                safety_checker=None,
                requires_safety_checker=False,
                use_safetensors=True
            )
            
            if torch.cuda.is_available():
                self.models["image"] = self.models["image"].to("cuda")
                # å¯ç”¨å†…å­˜ä¼˜åŒ–
                self.models["image"].enable_attention_slicing()
                self.models["image"].enable_vae_slicing()
            
            logger.info("âœ… å›¾åƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å›¾åƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.models["image"] = None
    
    async def _init_music_model(self):
        """åˆå§‹åŒ–éŸ³ä¹ç”Ÿæˆæ¨¡å‹"""
        try:
            logger.info("ğŸµ åŠ è½½éŸ³ä¹ç”Ÿæˆæ¨¡å‹...")
            
            # éŸ³ä¹æ¨¡å‹è¾ƒå¤§ï¼Œæš‚æ—¶è·³è¿‡æˆ–ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            logger.info("â­ï¸ éŸ³ä¹æ¨¡å‹æš‚æ—¶è·³è¿‡ (éœ€è¦å¤§é‡å†…å­˜)")
            self.models["music"] = None
            
        except Exception as e:
            logger.error(f"âŒ éŸ³ä¹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.models["music"] = None
    
    async def _init_code_model(self):
        """åˆå§‹åŒ–ä»£ç ç”Ÿæˆæ¨¡å‹"""
        try:
            logger.info("ğŸ’» åŠ è½½ä»£ç ç”Ÿæˆæ¨¡å‹...")
            
            from transformers import pipeline
            
            config = self.model_configs["code"]
            
            # ä½¿ç”¨è¾ƒå°çš„ä»£ç æ¨¡å‹
            try:
                self.models["code"] = pipeline(
                    "text-generation",
                    model="microsoft/CodeGPT-small-py",
                    device=0 if self.device == "cuda" and torch.cuda.is_available() else -1,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
                )
                logger.info("âœ… ä»£ç æ¨¡å‹åŠ è½½æˆåŠŸ")
            except:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ–‡æœ¬æ¨¡å‹
                self.models["code"] = self.models.get("text")
                logger.info("âœ… ä½¿ç”¨æ–‡æœ¬æ¨¡å‹ä½œä¸ºä»£ç ç”Ÿæˆå¤‡ç”¨")
            
        except Exception as e:
            logger.error(f"âŒ ä»£ç æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.models["code"] = self.models.get("text")
    
    async def generate_text(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.models.get("text"):
            return f"åŸºäºæç¤ºã€Œ{prompt}ã€çš„AIç”Ÿæˆæ–‡æœ¬å†…å®¹ã€‚(æ–‡æœ¬æ¨¡å‹æœªåŠ è½½ï¼Œè¿™æ˜¯æ¨¡æ‹Ÿè¾“å‡º)"
        
        try:
            max_length = min(kwargs.get("max_length", 200), 1000)
            temperature = kwargs.get("temperature", 0.7)
            
            # æ„å»ºæ›´å¥½çš„æç¤º
            formatted_prompt = f"ç”¨æˆ·: {prompt}\nåŠ©æ‰‹: "
            
            result = self.models["text"](
                formatted_prompt,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.models["text"].tokenizer.eos_token_id,
                num_return_sequences=1,
                truncation=True
            )
            
            generated_text = result[0]["generated_text"]
            
            # æå–åŠ©æ‰‹å›å¤éƒ¨åˆ†
            if "åŠ©æ‰‹: " in generated_text:
                generated_text = generated_text.split("åŠ©æ‰‹: ", 1)[1].strip()
            elif formatted_prompt in generated_text:
                generated_text = generated_text.replace(formatted_prompt, "").strip()
            
            return generated_text or f"åŸºäºã€Œ{prompt}ã€ç”Ÿæˆçš„å†…å®¹ã€‚"
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return f"åŸºäºæç¤ºã€Œ{prompt}ã€çš„AIç”Ÿæˆæ–‡æœ¬å†…å®¹ã€‚(ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯)"
    
    async def generate_image(self, prompt: str, **kwargs) -> bytes:
        """ç”Ÿæˆå›¾åƒ"""
        if not self.models.get("image"):
            # è¿”å›å ä½ç¬¦å›¾åƒURL
            raise ValueError("å›¾åƒæ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®")
        
        try:
            width = min(kwargs.get("width", 512), 768)
            height = min(kwargs.get("height", 512), 768)
            steps = min(kwargs.get("steps", 20), 50)
            guidance_scale = kwargs.get("guidance_scale", 7.5)
            
            # ä¼˜åŒ–æç¤ºè¯
            enhanced_prompt = f"{prompt}, high quality, detailed, masterpiece"
            
            image = self.models["image"](
                enhanced_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=guidance_scale,
                negative_prompt="blurry, low quality, distorted"
            ).images[0]
            
            # è½¬æ¢ä¸ºbytes
            import io
            img_bytes = io.BytesIO()
            image.save(img_bytes, format='PNG', quality=95)
            return img_bytes.getvalue()
            
        except Exception as e:
            logger.error(f"å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def generate_music(self, prompt: str, **kwargs) -> bytes:
        """ç”ŸæˆéŸ³ä¹"""
        # éŸ³ä¹ç”Ÿæˆæš‚æœªå®ç°
        raise NotImplementedError("éŸ³ä¹ç”ŸæˆåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…")
    
    async def generate_code(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆä»£ç """
        if not self.models.get("code"):
            return self._generate_code_template(prompt, kwargs.get("language", "python"))
        
        try:
            language = kwargs.get("language", "python")
            max_length = min(kwargs.get("max_length", 200), 500)
            
            # æ„å»ºä»£ç ç”Ÿæˆæç¤º
            code_prompt = f"# {prompt}\n# Language: {language}\n"
            
            result = self.models["code"](
                code_prompt,
                max_length=max_length,
                temperature=0.2,
                do_sample=True,
                pad_token_id=self.models["code"].tokenizer.eos_token_id
            )
            
            generated_code = result[0]["generated_text"]
            
            # æ¸…ç†è¾“å‡º
            if code_prompt in generated_code:
                generated_code = generated_code.replace(code_prompt, "").strip()
            
            return generated_code or self._generate_code_template(prompt, language)
            
        except Exception as e:
            logger.error(f"ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_code_template(prompt, language)
    
    def _generate_code_template(self, prompt: str, language: str) -> str:
        """ç”Ÿæˆä»£ç æ¨¡æ¿"""
        templates = {
            "python": f'''# {prompt}
def solution():
    """
    {prompt}
    """
    # TODO: å®ç°åŠŸèƒ½
    pass

if __name__ == "__main__":
    result = solution()
    print(result)''',
            
            "javascript": f'''// {prompt}
function solution() {{
    /**
     * {prompt}
     */
    // TODO: å®ç°åŠŸèƒ½
    return null;
}}

// è°ƒç”¨å‡½æ•°
const result = solution();
console.log(result);''',
            
            "go": f'''package main

import "fmt"

// {prompt}
func solution() {{
    // TODO: å®ç°åŠŸèƒ½
    fmt.Println("Hello, World!")
}}

func main() {{
    solution()
}}''',
        }
        
        return templates.get(language, f"// {prompt}\n// TODO: å®ç°åŠŸèƒ½")
    
    async def get_status(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹çŠ¶æ€"""
        return {
            "initialized": self._initialized,
            "device": self.device,
            "available_models": {
                "text": self.models.get("text") is not None,
                "image": self.models.get("image") is not None,
                "music": self.models.get("music") is not None,
                "code": self.models.get("code") is not None,
            },
            "model_configs": {
                name: {
                    "model_id": config["model_id"],
                    "local_exists": config["local_path"].exists(),
                    "type": config["type"]
                }
                for name, config in self.model_configs.items()
            },
            "memory_usage": self._get_memory_usage()
        }
    
    def _get_memory_usage(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            return {
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated() / 1e9:.2f}GB",
                "gpu_memory_reserved": f"{torch.cuda.memory_reserved() / 1e9:.2f}GB",
                "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB"
            }
        return {"cpu_only": True}
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†æ¨¡å‹èµ„æº...")
        
        for name, model in self.models.items():
            if model is not None:
                del model
        
        self.models.clear()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()
        logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
